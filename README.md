# Machine Learning Reading Group
![image](images/readmeImage.png)


Authors: @edwardyang12 @catherinelee274

We like to read machine learning papers (roughly) every week! We take turns leading the discussion.


We follow this structure: 
- What does the paper introduce
- Data 
- Training/Finetuning
- Evaluation
- Discussion


# Papers 
- [x] Formal Algorithms for Transformers
- [x] [Power of Scale For Prompt Finetuning](https://arxiv.org/abs/2104.08691)
- [x] Flash Attention
- [x] [Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (need to insert date)
- [x] Big Bird
- [x] Flash Attention
- [x] Power of Scale for prompt tuning
- [x] Nerf
- [x] Retrieval Augmented Generation
- [x] Survey on Deep Reinforcement Learning
- [x] [Latent Diffusion Models](https://arxiv.org/abs/2112.10752) 1/7/2023
- [x] Mixture of Experts
- [x] [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- [x] Mappo
- [x] Mamba
- [x] Contrastive Language-Image Pretraining 03/17/2024
- [x] [Deepseed Inference](https://arxiv.org/abs/2207.00032) 03/24/2024
- [x] [Llama](https://arxiv.org/abs/2302.13971) 03/31/2024
- [ ] [Multimodal Learning with Transformers](https://arxiv.org/abs/2206.06488) 4/14/2024
